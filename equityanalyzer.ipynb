{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import assets.config as config\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import re\n",
    "import numpy as np\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import seaborn as sns\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker = 'NFLX'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Daily Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(f'data/{ticker}_daily.csv'):\n",
    "    print('File exists')\n",
    "else:\n",
    "    # Define the URL and headers\n",
    "    url = f\"https://yahoo-finance127.p.rapidapi.com/historic/{ticker}/1d/730d\"\n",
    "    headers = {\n",
    "        \"x-rapidapi-key\": \"dfbe7c0a1fmshe393d06dc43c773p13ef07jsnc9ba1900f6ce\",\n",
    "        \"x-rapidapi-host\": \"yahoo-finance127.p.rapidapi.com\"\n",
    "    }\n",
    "\n",
    "    # Make the request\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Extract JSON data\n",
    "        data = response.json()\n",
    "\n",
    "        # Extract data\n",
    "        timestamps = data['timestamp']\n",
    "        close_prices = data['indicators']['quote'][0]['close']\n",
    "        volumes = data['indicators']['quote'][0]['volume']\n",
    "\n",
    "        # Convert timestamps to \"yyyy-mm-dd\" format\n",
    "        dates = [datetime.utcfromtimestamp(ts).strftime('%Y-%m-%d') for ts in timestamps]\n",
    "\n",
    "        # Prepare data for CSV\n",
    "        csv_data = list(zip(dates, close_prices, volumes))\n",
    "\n",
    "        # Define CSV file path\n",
    "        csv_file_path = f'data/{ticker}_daily.csv'\n",
    "\n",
    "        # Write to CSV file\n",
    "        with open(csv_file_path, mode='w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(['Date', 'Close', 'Volume'])  # Header row\n",
    "            writer.writerows(csv_data)\n",
    "\n",
    "        print(f\"Data written to {csv_file_path}\")\n",
    "    else:\n",
    "        print(\"Error:\", response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alphavantage implementation\n",
    "\n",
    "# url = f'https://www.alphavantage.co/query?function=TIME_SERIES_DAILY&symbol={ticker}&apikey={config.alphavantage_apikey}'\n",
    "# r = requests.get(url)\n",
    "# data = r.json()\n",
    "\n",
    "# header = list(data['Time Series (Daily)']['2024-05-14'].keys())\n",
    "# header.insert(0, 'Date')\n",
    "\n",
    "# # Writing to CSV\n",
    "# with open(f'data/{ticker}_daily.csv', 'w', newline='') as csvfile:\n",
    "#     writer = csv.DictWriter(csvfile, fieldnames=header)\n",
    "    \n",
    "#     writer.writeheader()\n",
    "    \n",
    "#     for date, values in data['Time Series (Daily)'].items():\n",
    "#         row = {'Date': date}\n",
    "#         row.update(values)\n",
    "#         writer.writerow(row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insider Trades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = f\"http://openinsider.com/screener?s={ticker}&o=&pl=&ph=&ll=&lh=&fd=730&fdr=&td=0&tdr=&fdlyl=&fdlyh=&daysago=&xp=1&xs=1&vl=&vh=&ocl=&och=&sic1=-1&sicl=100&sich=9999&grp=0&nfl=&nfh=&nil=&nih=&nol=&noh=&v2l=&v2h=&oc2l=&oc2h=&sortcol=0&cnt=1000&page=1\"\n",
    "\n",
    "# Request the webpage\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "time.sleep(1)\n",
    "\n",
    "# Find the table\n",
    "table = soup.find('table', class_='tinytable')\n",
    "\n",
    "if table:\n",
    "    # Extract table rows\n",
    "    rows = table.find_all('tr')\n",
    "\n",
    "    # Create an empty list to store the data\n",
    "    data = []\n",
    "\n",
    "    # Loop through rows and extract data\n",
    "    for row in rows[1:]:  # Skip the header row\n",
    "        cells = row.find_all('td')\n",
    "        insider_info = {\n",
    "            'Trade Date': cells[2].get_text(strip=True),\n",
    "            'Value': cells[11].get_text(strip=True)\n",
    "        }\n",
    "        data.append(insider_info)\n",
    "\n",
    "    # Convert the list of dictionaries to a DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Remove dollar sign and comma from 'Value' column and convert to numeric type\n",
    "    df['Value'] = df['Value'].str.replace('$', '').str.replace(',', '').astype(float)\n",
    "\n",
    "    # Group the data by 'Trade Date' and aggregate the values\n",
    "    df_merged = df.groupby('Trade Date').agg({'Value': 'sum'}).reset_index()\n",
    "\n",
    "    # Save DataFrame to CSV\n",
    "    df_merged.to_csv(f'data/{ticker}_insider_trades.csv', index=False)\n",
    "    print('Insider trades data saved')\n",
    "\n",
    "else:\n",
    "    print('No table found')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "article_links = []\n",
    "\n",
    "def get_yahoo_articles():\n",
    "    driver.get(f\"https://finance.yahoo.com/quote/{ticker}/news\")\n",
    "\n",
    "    # Scroll down the page multiple times to load more articles\n",
    "    for _ in range(5): # scroll x times\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(1)\n",
    "\n",
    "    page_source = driver.page_source\n",
    "    driver.quit()\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "    articles = soup.find_all(\"li\", class_=\"stream-item svelte-7rcxn\")\n",
    "\n",
    "    for article in articles:\n",
    "        link = article.find(\"a\").get(\"href\")\n",
    "        if '/news/' in link: article_links.append(link)\n",
    "        \n",
    "get_yahoo_articles()\n",
    "\n",
    "print(article_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_yahoo_finance(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Get header\n",
    "        header_element = soup.find(id=\"caas-lead-header-undefined\")\n",
    "        header = header_element.get_text().strip()\n",
    "\n",
    "        # Get body\n",
    "        article_elements = soup.find_all(class_=\"caas-body\")\n",
    "        body = \"\"\n",
    "        for element in article_elements: body += element.get_text().strip() + \"\\n\"\n",
    "\n",
    "        # Get date\n",
    "        date_text = soup.find_all(class_=\"caas-attr-time-style\")[0].get_text().strip()\n",
    "        date = re.search(r'(\\w{3}, \\w{3} \\d{1,2}, \\d{4})', date_text)\n",
    "        date = datetime.strptime(date.group(), \"%a, %b %d, %Y\").strftime(\"%Y-%m-%d\")\n",
    "\n",
    "        return header, body, date\n",
    "    else:\n",
    "        print(\"Failed to retrieve the webpage. Status code:\", response.status_code)\n",
    "\n",
    "def analyze(text):\n",
    "    # Summarizer\n",
    "    summarizer_api_url = \"https://api-inference.huggingface.co/models/sshleifer/distilbart-cnn-12-6\"\n",
    "    summarizer_headers = {f\"Authorization\": f\"Bearer {config.huggingface_apikey}\"}\n",
    "    summarizer_output = requests.post(summarizer_api_url, headers=summarizer_headers, json={\"inputs\": text}).json()\n",
    "    text = summarizer_output[0]['summary_text']\n",
    "\n",
    "    # Sentimentizer\n",
    "    sentimentizer_api_url = \"https://api-inference.huggingface.co/models/ProsusAI/finbert\"\n",
    "    sentimentizer_headers = {\"Authorization\": f\"Bearer {config.huggingface_apikey}\"}\n",
    "    sentimentizer_output = requests.post(sentimentizer_api_url, headers=sentimentizer_headers, json={\"inputs\": text}).json()\n",
    "\n",
    "    most_likely_label, max_score = None, 0\n",
    "    for sentiment in sentimentizer_output[0]:\n",
    "        label, score = sentiment['label'], sentiment['score']\n",
    "        if score > max_score: most_likely_label, max_score = label, score\n",
    "    if most_likely_label == 'positive': sentiment = 1\n",
    "    elif most_likely_label == 'neutral': sentiment = 0\n",
    "    else: sentiment = -1\n",
    "\n",
    "    return sentiment, summarizer_output, sentimentizer_output\n",
    "\n",
    "# Main engine\n",
    "articledata = []\n",
    "company_table = pd.read_csv('assets/companies.csv')\n",
    "short_name, company_name = company_table.loc[company_table['ticker'] == ticker]['short name'].values[0], company_table.loc[company_table['ticker'] == ticker]['company name'].values[0]\n",
    "\n",
    "for i, article in enumerate(article_links):\n",
    "    \n",
    "    try: \n",
    "        head, body, date = scrape_yahoo_finance(article)\n",
    "        if ticker in head or short_name in head or company_name in head: pass\n",
    "        else: continue\n",
    "    except Exception as scrape_error:\n",
    "        print(f\"Failed to scrape article '{article}': {scrape_error.__class__.__name__}: {str(scrape_error)} \\n\")\n",
    "        continue\n",
    "\n",
    "    try: \n",
    "        sentiment, summarizer_output, sentimentizer_output = analyze(body)\n",
    "    except Exception as sentiment_error:\n",
    "        print(f\"Failed to analyze sentiment for article '{article}': {sentiment_error.__class__.__name__}: {str(sentiment_error)} \\n\")\n",
    "        continue\n",
    "\n",
    "    articledata.append({'Date': date, 'Sentiment': sentiment})\n",
    "    print(f\"Iteration {i}: {date}, '{head}'\\n- {article}\\n- Summarizer output:{summarizer_output[0]['summary_text']}\\n- Sentimentizer output: {sentimentizer_output[0][0]['label']} ({round(sentimentizer_output[0][0]['score'], 2)})\\n\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(articledata)\n",
    "\n",
    "# Initialize new sentiment columns\n",
    "df['Negative_Count'] = (df['Sentiment'] == -1).astype(int)\n",
    "df['Neutral_Count'] = (df['Sentiment'] == 0).astype(int)\n",
    "df['Positive_Count'] = (df['Sentiment'] == 1).astype(int)\n",
    "df['Negative_Count'] *= -1\n",
    "\n",
    "# Group by date and sum the counts\n",
    "df_grouped = df.groupby('Date').agg({\n",
    "    'Negative_Count': 'sum',\n",
    "    'Neutral_Count': 'sum',\n",
    "    'Positive_Count': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "# Add a total count column\n",
    "df_grouped['Total_Count'] = df_grouped['Negative_Count'] + df_grouped['Positive_Count']\n",
    "\n",
    "# Save to CSV\n",
    "df_grouped.to_csv(f'data/{ticker}_sentiment.csv', index=False)\n",
    "\n",
    "print(\"Success\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "historical = pd.read_csv(f'data/{ticker}_daily.csv')\n",
    "insider = pd.read_csv(f'data/{ticker}_insider_trades.csv')\n",
    "news = pd.read_csv(f'data/{ticker}_sentiment.csv')\n",
    "\n",
    "newdata = pd.merge(historical, insider, how='left', left_on='Date', right_on='Trade Date')\n",
    "newdata = pd.merge(newdata, news, how='left', on='Date')\n",
    "newdata['Date'] = pd.to_datetime(newdata['Date'])\n",
    "\n",
    "# Alphavantage technique: newdata.rename(columns={'4. close': 'Close', '5. volume': 'Volume', 'Value': 'Insider_Trades'}, inplace=True)\n",
    "# Alphavantage technique: newdata = newdata.drop(columns=['1. open', '2. high', '3. low', 'Trade Date'])\n",
    "newdata.rename(columns={'Value': 'Insider_Trades'}, inplace=True)\n",
    "newdata = newdata.fillna(0)\n",
    "newdata['Close_Delta'] = newdata['Close'] - newdata['Close'].shift(1)\n",
    "# newdata['Total_Count_Delta'] = newdata['Total_Count'] - newdata['Total_Count'].shift(1)\n",
    "# newdata['Insider_Trades'] = newdata['Insider_Trades'].fillna(0)\n",
    "\n",
    "newdata = newdata.drop(columns=['Trade Date'])\n",
    "\n",
    "newdata.to_csv(f'data/{ticker}_merged.csv', index=True)\n",
    "\n",
    "print(newdata.head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 6), sharex=True)\n",
    "\n",
    "ax1.plot(newdata['Date'], newdata['Insider_Trades'], color='blue')\n",
    "ax2.plot(newdata['Date'], newdata['Close'], color='green')\n",
    "\n",
    "ax1.set_ylabel('Insider Trades')\n",
    "ax2.set_ylabel('Close Price')\n",
    "ax2.set_xlabel('Date')\n",
    "plt.suptitle('Insider Trades vs Close Price')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(newdata['Close'], newdata['Total_Count'])\n",
    "# print(np.corrcoef(newdata['Close'], newdata['Total_Count'])[0, 1])\n",
    "# print(newdata['Date'][::-1][:(int(len(news) * 1.2))])\n",
    "\n",
    "newdata = newdata[::-1][:int(len(news) * 1.2)]\n",
    "# print(newdata['Total_Count'])\n",
    "newdata['Total_Count'] = newdata['Total_Count'].shift(periods=1)\n",
    "newdata['Negative_Count'] = newdata['Total_Count'].shift(periods=1)\n",
    "newdata['Positive_Count'] = newdata['Total_Count'].shift(periods=1)\n",
    "# print(newdata)[:5]\n",
    "\n",
    "# Convert 'Date' column to datetime\n",
    "newdata['Date'] = pd.to_datetime(newdata['Date'])\n",
    "\n",
    "# Create subplots\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 6), sharex=True)\n",
    "\n",
    "ax1.bar(newdata['Date'], newdata['Negative_Count'], color='red', align='center', label='Negative Sentiment')\n",
    "ax1.bar(newdata['Date'], newdata['Positive_Count'], color='green', align='center', label='Positive Sentiment')\n",
    "# ax1.bar(newdata['Date'], newdata['Total_Count'], color='blue', align='center', label='Overall Sentiment')\n",
    "\n",
    "# Add horizontal gridlines to the top chart\n",
    "ax1.grid(which='major', axis='y', linestyle='--', linewidth=0.5)\n",
    "ax2.grid(which='major', axis='y', linestyle='--', linewidth=0.5)\n",
    "ax1.axhline(0, color='black')\n",
    "\n",
    "# Plot Close Price\n",
    "ax2.plot(newdata['Date'], newdata['Close'], color='darkblue', label='Close Price')\n",
    "\n",
    "# Set y-axis labels\n",
    "ax1.set_ylabel('Sentiment Count')\n",
    "ax2.set_ylabel('Close Price')\n",
    "\n",
    "# Set x-axis label\n",
    "ax2.set_xlabel('Date')\n",
    "\n",
    "# Add legend to ax1\n",
    "ax1.legend()\n",
    "\n",
    "# Set title for the entire plot\n",
    "plt.suptitle('News Sentiment vs Close Price')\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data_path = 'subset.csv'\n",
    "newdata = pd.read_csv(data_path)\n",
    "print(newdata)\n",
    "\n",
    "newdata['Date'] = pd.to_datetime(newdata['Date'])\n",
    "\n",
    "newdata['Total_Count'] = newdata['Total_Count'].shift(1)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(newdata['Date'], newdata['Total_Count'], label='Total_Count', color='blue')\n",
    "plt.plot(newdata['Date'], newdata['Close_Delta'], label='Close Price', color='green')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
