{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ryanhermes/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import config\n",
    "import torch\n",
    "import csv\n",
    "from datetime import datetime, date, timedelta\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "from transformers import pipeline\n",
    "import numpy as np\n",
    "import re\n",
    "from scipy.special import softmax\n",
    "\n",
    "ticker = 'AAPL'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Daily Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = f'https://www.alphavantage.co/query?function=TIME_SERIES_DAILY&symbol={ticker}&apikey={config.alphavantage_apikey}'\n",
    "r = requests.get(url)\n",
    "data = r.json()\n",
    "\n",
    "header = list(data['Time Series (Daily)']['2024-05-14'].keys())\n",
    "header.insert(0, 'Date')\n",
    "\n",
    "# Writing to CSV\n",
    "with open(f'data/{ticker}_daily.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=header)\n",
    "    \n",
    "    writer.writeheader()\n",
    "    \n",
    "    for date, values in data['Time Series (Daily)'].items():\n",
    "        row = {'Date': date}\n",
    "        row.update(values)\n",
    "        writer.writerow(row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insider Trades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insider trades data saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b7/3syr39bd24n_5m711ln6vz800000gn/T/ipykernel_33791/1697392439.py:30: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  df['Value'] = df['Value'].str.replace('$', '').str.replace(',', '').astype(float)\n"
     ]
    }
   ],
   "source": [
    "url = f\"http://openinsider.com/search?q={ticker}\"\n",
    "\n",
    "# Request the webpage\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Find the table\n",
    "table = soup.find('table', class_='tinytable')\n",
    "\n",
    "if table:\n",
    "    # Extract table rows\n",
    "    rows = table.find_all('tr')\n",
    "\n",
    "    # Create an empty list to store the data\n",
    "    data = []\n",
    "\n",
    "    # Loop through rows and extract data\n",
    "    for row in rows[1:]:  # Skip the header row\n",
    "        cells = row.find_all('td')\n",
    "        insider_info = {\n",
    "            'Trade Date': cells[2].get_text(strip=True),\n",
    "            'Value': cells[11].get_text(strip=True)\n",
    "        }\n",
    "        data.append(insider_info)\n",
    "\n",
    "    # Convert the list of dictionaries to a DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Remove dollar sign and comma from 'Value' column and convert to numeric type\n",
    "    df['Value'] = df['Value'].str.replace('$', '').str.replace(',', '').astype(float)\n",
    "\n",
    "    # Group the data by 'Trade Date' and aggregate the values\n",
    "    df_merged = df.groupby('Trade Date').agg({'Value': 'sum'}).reset_index()\n",
    "\n",
    "    # Save DataFrame to CSV\n",
    "    df_merged.to_csv(f'data/{ticker}_insider_trades.csv', index=False)\n",
    "    print('Insider trades data saved')\n",
    "\n",
    "else:\n",
    "    print('No table found')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to retrieve the webpage. Status code: 404\n",
      "Articles sentiment data combined and saved\n"
     ]
    }
   ],
   "source": [
    "# List of articles to scrape\n",
    "articles = [\n",
    "    \"https://finance.yahoo.com/news/apple-touts-stopping-1-8bn-170000332.html\",\n",
    "    \"https://finance.yahoo.com/news/openai-leap-forward-human-ai-150259365.html\",\n",
    "    \"https://finance.yahoo.com/m/df9a8cd8-0f31-39bc-909d-d877af2ff523/heard-on-the-street-apple-s.html\",\n",
    "    \"https://finance.yahoo.com/news/where-apple-stock-5-years-183000424.html\",\n",
    "    \"https://finance.yahoo.com/m/6104f210-fff1-3b57-a907-5f578b487d65/apple-makes-rare-apology-for.html\"\n",
    "]\n",
    "\n",
    "# Function to scrape Yahoo Finance article\n",
    "def scrape_yahoo_finance(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        # Get header\n",
    "        header_element = soup.find(id=\"caas-lead-header-undefined\")\n",
    "        header = header_element.get_text().strip()\n",
    "        # Get body\n",
    "        article_elements = soup.find_all(class_=\"caas-body\")\n",
    "        body = \"\"\n",
    "        for element in article_elements:\n",
    "            body += element.get_text().strip() + \"\\n\"\n",
    "        # Get date\n",
    "        date_text = soup.find_all(class_=\"caas-attr-time-style\")[0].get_text().strip()\n",
    "        date = datetime.strptime(date_text[:17], \"%a, %b %d, %Y\").strftime(\"%Y-%m-%d\")\n",
    "        return header + \". \" + body, date\n",
    "    else:\n",
    "        print(\"Failed to retrieve the webpage. Status code:\", response.status_code)\n",
    "        return None, None\n",
    "\n",
    "# Function to query sentiment analysis models\n",
    "def query_sentiment_analysis(text):\n",
    "    # Summarizer\n",
    "    summarizer_api_url = \"https://api-inference.huggingface.co/models/sshleifer/distilbart-cnn-12-6\"\n",
    "    summarizer_headers = {f\"Authorization\": f\"Bearer {config.huggingface_apikey}\"}\n",
    "    summarizer_output = requests.post(summarizer_api_url, headers=summarizer_headers, json={\"inputs\": text}).json()\n",
    "    summarized_text = summarizer_output[0]['summary_text']\n",
    "    \n",
    "    # Finbert\n",
    "    finbert_api_url = \"https://api-inference.huggingface.co/models/ProsusAI/finbert\"\n",
    "    finbert_headers = {\"Authorization\": f\"Bearer {config.huggingface_apikey}\"}\n",
    "    finbert_output = requests.post(finbert_api_url, headers=finbert_headers, json={\"inputs\": summarized_text}).json()\n",
    "    \n",
    "    # Extract the list of sentiments\n",
    "    sentiments = finbert_output[0]\n",
    "    # Initialize variables to store the most likely sentiment label and score\n",
    "    most_likely_label = None\n",
    "    max_score = -1\n",
    "    # Iterate through the list of sentiments\n",
    "    for sentiment in sentiments:\n",
    "        label = sentiment['label']\n",
    "        score = sentiment['score']\n",
    "        # Update most_likely_label and max_score if the current score is higher\n",
    "        if score > max_score:\n",
    "            most_likely_label = label\n",
    "            max_score = score\n",
    "    # Convert sentiment label to numerical value\n",
    "    if most_likely_label == 'positive':\n",
    "        numerical_sentiment = 1\n",
    "    elif most_likely_label == 'neutral':\n",
    "        numerical_sentiment = 0\n",
    "    elif most_likely_label == 'negative':\n",
    "        numerical_sentiment = -1\n",
    "    return numerical_sentiment\n",
    "\n",
    "# Initialize list to store article data\n",
    "articledata = []\n",
    "\n",
    "# Iterate through the list of articles\n",
    "for article in articles:\n",
    "    # Scrape article content and date\n",
    "    article_text, article_date = scrape_yahoo_finance(article)\n",
    "    if article_text is not None and article_date is not None:\n",
    "        # Analyze sentiment and convert to numerical value\n",
    "        numerical_sentiment = query_sentiment_analysis(article_text)\n",
    "        # Append article date and sentiment to articledata list\n",
    "        articledata.append({'Date': article_date, 'Sentiment': numerical_sentiment})\n",
    "\n",
    "# Convert article data to DataFrame\n",
    "df = pd.DataFrame(articledata)\n",
    "\n",
    "# Group by date and sum the sentiments\n",
    "df_grouped = df.groupby('Date').sum().reset_index()\n",
    "\n",
    "# Save DataFrame to CSV\n",
    "df_grouped.to_csv(f'data/{ticker}_sentiment.csv', index=False)\n",
    "\n",
    "print(\"Articles sentiment data combined and saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "article = \"https://finance.yahoo.com/news/apple-touts-stopping-1-8bn-170000332.html\"\n",
    "articledata = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple touts stopping $1.8BN in App Store fraud last year in latest pitch to developers. Apple released new data about anti-fraud measures related to its operation of the iOS App Store on Tuesday morning, trumpeting a claim that it stopped over $7 billion in \"potentially fraudulent transactions\" across the four years between 2020 and 2023.More than $1.8 billion of that total was stopped in 2023, per Apple, which is down from the $2 billion in potentially fraudulent transactions Apple reported preventing in 2022. It also said it blocked over 14 million stolen credit cards and more than 3.3 million accounts from transacting again between 2020 and 2023.As with any self-reported corporate metrics, the aim is to shape a narrative: In Apple's case it's a longstanding claim that its mobile ecosystem sets \"the standard for security, reliability, and user experience\", as its blog post puts it.It's worth noting that counter-narratives do exist, such as the developer lawsuit Apple settled back in fall 2022, which had raised complaints about unfair app rejections, scams, and fraud.The timing of Apple's blog post coincides -- coincidentally or not -- with the kick-off of Google's developer confab, I/O. That's interesting because, in recent months, Mountain View has been running a pilot of a new automated anti-fraud measure for its own app store, Google Play, suggesting competition to burnish mobile security cred is heating up thanks to AI.Apple's other pressure point on ecosystem integrity comes from regulators. In the European Union the iPhone maker has, since February, been forced into allowing third party app stores and app sideloading under the bloc's Digital Markets Act (DMA). It must also let developers use third party payment tech (rather than its own) if they wish. Apple argues the DMA's enforced openness is weakening the security of its iOS ecosystem.The \"fourth annual fraud prevention analysis\" Apple has published today offers a retrospective look at where its App Store ecosystem stood on stopping scams and other problematic behaviors before meddling EU regulators got involved.It also reads like a marketing pitch to developers who, in the EU at least, have an increasing array of choices about how to distribute their apps, rather than being forced to submit to the Apple's App Store to reach iOS users.App Store integrity in the frameReporting additional metrics for 2023, Apple said it rejected more than 1.7 million app submissions for failing to meet its \"stringent\" standards for privacy, security, and content. It also said its efforts to stop and reduce App Store fraud led to it terminating nearly 374 million developer and customer accounts, and removing \"close to\" 152 milllion ratings and reviews over fraud concerns.Story continuesAlso in 2023, Apple said it shuttered close to 118,000 developer accounts -- which its blog post notes is a marked decrease from the 428,000 terminations in the prior year (2022). It credits \"continued improvements\" in preventing the creation of potentially fraudulent accounts in the first place with this decrease, without specifying the changes it's made.In further actions last year, Apple said it rejected more than 91,000 developer enrollments for \"fraud concerns\" -- preventing these accounts from submitting what it couched as \"problematic apps\" to the App Store.Apple says is App Review team has over 500 staff who are tasked with evaluating every app submission. \"On average, the team reviews approximately 132,500 apps a week, and in 2023, reviewed nearly 6.9M app submissions while helping more than 192,000 developers publish their first app onto the App Store,\" it wrote.Apple said its App Review workflow involves both automated processes and human review to try to spot and block fraud and other harms. In 2023, more than 1.7 million app submissions were rejected by Apple for \"various reasons, including privacy violations and fraudulent activity\".\"Bad actors employ deceptive tactics to harm users, including the practice of disguising potentially risky apps as innocuous ones,\" the company wrote. \"Over the past year, there have been numerous instances where App Review identified apps initially misrepresented as harmless products -- such as photo editors or puzzle games -- that later transformed postreview into pirate movie streaming platforms, illegal gambling apps, or fraudulent and predatory loan issuers,\" it wrote.\"In some extreme instances, the team also identified and removed financial service apps involved in complex and malicious social engineering efforts designed to defraud users, including apps impersonating known services to facilitate phishing campaigns and that provided fraudulent financial and investment services,\" Apple added, noting App Store reviewers \"removed or rejected 40,000 apps from developers who engaged in bait-and-switch activity\" across the year.This article originally appeared on TechCrunch at https://techcrunch.com/2024/05/14/apple-touts-stopping-1-8bn-in-app-store-fraud-last-year-in-latest-pitch-to-devs/View comments\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def scrape_yahoo_finance(url):\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Get header\n",
    "        header_element = soup.find(id=\"caas-lead-header-undefined\")\n",
    "        header = header_element.get_text().strip()\n",
    "        \n",
    "        # Get body\n",
    "        article_elements = soup.find_all(class_=\"caas-body\")\n",
    "        body = \"\"\n",
    "        for element in article_elements:\n",
    "            body += element.get_text().strip() + \"\\n\"\n",
    "\n",
    "        # Get date\n",
    "        date_text = soup.find_all(class_=\"caas-attr-time-style\")[0].get_text().strip()\n",
    "        date = datetime.strptime(date_text[:17], \"%a, %b %d, %Y\").strftime(\"%Y-%m-%d\")\n",
    "        articledata.append(date)\n",
    "        # print(date)\n",
    "        \n",
    "        # text = header + \"\\n\" + body\n",
    "        text = header + \". \" + body\n",
    "        return text\n",
    "    \n",
    "    else:\n",
    "        print(\"Failed to retrieve the webpage. Status code:\", response.status_code)\n",
    "        return None\n",
    "\n",
    "text = scrape_yahoo_finance(article)\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarize (if necessary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Summarizer 1\n",
    "\n",
    "# print(text)\n",
    "# print(type(text))\n",
    "\n",
    "# # text = f'Provide a summary for this article, keeping it relevant to {ticker}. I want the summary to revolve around {ticker}.' + text\n",
    "\n",
    "# if len(text.split()) > 300:\n",
    "#     summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "#     # print(summarizer(text, max_length=200, min_length=30, do_sample=False))\n",
    "#     x = summarizer(text, max_length=200, min_length=30, do_sample=False)\n",
    "\n",
    "#     print(x)\n",
    "\n",
    "#     text = summarizer(text, max_length=200, min_length=30, do_sample=False)[0]['summary_text']\n",
    "#     print('Summarized')\n",
    "\n",
    "# else: print('Not summarized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Summarizer 2\n",
    "\n",
    "# summarizer = pipeline(\"summarization\", \"jordiclive/flan-t5-3b-summarizer\", torch_dtype=torch.bfloat16)\n",
    "\n",
    "# results = summarizer(\n",
    "#         text,\n",
    "#         num_beams=5,\n",
    "#         min_length=5,\n",
    "#         no_repeat_ngram_size=3,\n",
    "#         truncation=True,\n",
    "#         max_length=200,\n",
    "#     )\n",
    "\n",
    "# print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Summarizer 3\n",
    "\n",
    "# summarizer = pipeline(\"summarization\", model=\"philschmid/bart-large-cnn-samsum\")\n",
    "\n",
    "# summarizer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Apple touts stopping $1.8BN in App Store fraud last year in latest pitch to developers . Apple says it blocked over 14 million stolen credit cards and more than 3.3 million accounts from transacting again . In 2023, Apple said it rejected more than 1.7 million app submissions for failing to meet \"stringent\" standards for privacy, security, and content .\n"
     ]
    }
   ],
   "source": [
    "# Summarizer 4\n",
    "\n",
    "API_URL = \"https://api-inference.huggingface.co/models/sshleifer/distilbart-cnn-12-6\"\n",
    "headers = {f\"Authorization\": f\"Bearer {config.huggingface_apikey}\"}\n",
    "\n",
    "def query(payload):\n",
    "\tresponse = requests.post(API_URL, headers=headers, json=payload)\n",
    "\treturn response.json()\n",
    "\t\n",
    "output = query({\n",
    "\t\"inputs\": text,\n",
    "})\n",
    "\n",
    "text = output[0]['summary_text']\n",
    "print(output[0]['summary_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'label': 'negative', 'score': 0.9409266710281372}, {'label': 'neutral', 'score': 0.045905645936727524}, {'label': 'positive', 'score': 0.013167675584554672}]]\n",
      "Most likely sentiment: negative\n",
      "Numerical sentiment: -1\n"
     ]
    }
   ],
   "source": [
    "API_URL = \"https://api-inference.huggingface.co/models/ProsusAI/finbert\"\n",
    "headers = {\"Authorization\": f\"Bearer {config.huggingface_apikey}\"}\n",
    "\n",
    "def query(payload):\n",
    "\tresponse = requests.post(API_URL, headers=headers, json=payload)\n",
    "\treturn response.json()\n",
    "\n",
    "output = query({\n",
    "\t\"inputs\": text,\n",
    "})\n",
    "\n",
    "print(output)\n",
    "\n",
    "# Extract the list of sentiments\n",
    "sentiments = output[0]\n",
    "\n",
    "# Initialize variables to store the most likely sentiment label and score\n",
    "most_likely_label = None\n",
    "max_score = -1\n",
    "\n",
    "# Iterate through the list of sentiments\n",
    "for sentiment in sentiments:\n",
    "    label = sentiment['label']\n",
    "    score = sentiment['score']\n",
    "    \n",
    "    # Update most_likely_label and max_score if the current score is higher\n",
    "    if score > max_score:\n",
    "        most_likely_label = label\n",
    "        max_score = score\n",
    "\n",
    "print(\"Most likely sentiment:\", most_likely_label)\n",
    "\n",
    "if most_likely_label == 'positive':\n",
    "    numerical_sentiment = 1\n",
    "elif most_likely_label == 'neutral':\n",
    "    numerical_sentiment = 0\n",
    "elif most_likely_label == 'negative':\n",
    "    numerical_sentiment = -1\n",
    "\n",
    "print(\"Numerical sentiment:\", numerical_sentiment)\n",
    "\n",
    "articledata.append(numerical_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2024-05-14', -1]\n"
     ]
    }
   ],
   "source": [
    "print(articledata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "historical = pd.read_csv(f'{ticker}_daily.csv')\n",
    "insider = pd.read_csv(f'{ticker}_insider_trades.csv')\n",
    "news = pd.read_csv(f'{ticker}_sentiment.csv')\n",
    "\n",
    "newdata = pd.merge(historical, insider, how='outer', left_on='Date', right_on='Trade Date')\n",
    "newdata = pd.merge(newdata, news, how='outer', on='Date')\n",
    "\n",
    "newdata.rename(columns={'4. close': 'Close', '5. volume': 'Volume', 'Value': 'Insider_Trades'}, inplace=True)\n",
    "newdata = newdata.drop(columns=['1. open', '2. high', '3. low', 'Trade Date'])\n",
    "\n",
    "newdata.to_csv(f'data/{ticker}_merged.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
